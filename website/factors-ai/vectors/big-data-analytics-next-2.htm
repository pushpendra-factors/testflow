<link rel="stylesheet" href="assets/css/blog.css">
 <!-- Blog Section Start  -->
 <div id="blog-single">
    <div class="container">
      <div class="row justify-content-center">
        <div class="col-lg-12 col-md-16 col-xs-16">
          <div class="blog-post">
            <div class="post-content">
              <h3>Big Data and Analytics - What's next?  (Part 2)</h3>
              <p>
                  In the last blog, we briefly went over the history of Big Data Technologies. How databases evolved from relational databases to nosql databases like Bigtable, Cassandra, DynamoDb etc with the rise of internet along with development of technologies like GFS, MapReduce etc for distributed file storage and computation. These technologies were first developed by companies like Google, Amazon etc and picked up in a big way by the open source community.
              </p>
              <br>
              <div class="post-thumb">
                  <img src="assets/img/blog/big_data_technologies.png" alt="">
              </div>
              <br>
              <h5>Big Data and Enterprises</h5>
              <p>
                  Soon enough commercial versions of these open source technologies were being distributed by companies like Cloudera, Hortonworks etc. Traditional enterprises started adopting these technologies for their analytics and reporting needs in a big way.
              </p>
              <br>
              <p>
                  Prior to this enterprises built data warehouses based on large relational databases. It involved combining data from multiple data sources like ERP, CRM etc and build an unified and relatively denormalized view of all of the data periodically. Design of such systems was involved and data was updated only periodically. Updating involved a three stage process of extracting data from various sources, combining and transforming these to the denormalized format and loading it into the data warehouse. This came to known as ETL (Extract, Transform and Load).
              </p>
              <br>
              <p>
                  With Hadoop, enterprises could just periodically dump all their data into a cluster of machines known as Data lakes and run ad-hoc run map reduces to pull out any report of interest. Visualization tools like Tableau and Qlik started connect directly to this ecosystem making it seamless to plot graphs from a simple interface, but actually generated by crunching large volumes of data in the background.
              </p>
              <br>
              <h5>Customer Centric View of Data</h5>
              <p>
                  Databases are a final system of record and analytics on databases only gives information on the current state of customers and not how they reached here.  With the rise of internet a lot of businesses are now online, or have multiple online / digital touchpoints with customers. It’s easier to instrument and collect customer data as a series of actions, be it clickstream or online transactions. This customer centric model of data enables richer analytics and insights. Additionally the data is incremental, and can be made available immediately in reports without having to go through the complex and periodic process of ETL. More enterprises are moving to this model and datastores and technologies to cater specifically these kind of use cases are being developed. Ex: TimescaleDB, Druid, Snowplow.
              </p>
              <br>
              <div class="post-thumb">
                  <img src="assets/img/blog/customer_centre.png" alt="">
              </div>
              <br>
              <h5>So what’s next?</h5>
              <p>
                  To summarize, the bulk of the big data revolution, that has happened in the last 15 years, is to build systems capable of storing and querying large amounts of data. The queries are raw i.e if X and Y are variables in the data and x1 and y2 are two corresponding values of interest, then the system can return all data points where in the variable X matches x1 and Y matches y2. Or some post processed result on all the matching data points. Along the way, we also have systems that can compute on large amounts of data in a distributed fashion.
              </p>
              <br>
              <p>
                  So what’s next in analytics from here? Is it building machine learning models? Certainly, the availability of all these data enables organizations to build predictive models for specific use cases. In fact, the recent surge of interest in machine learning has actually been because of the better results we get by running the old ML algorithms at larger scale in a distributed way. But while Machine Learning is useful to build offline predictive models to build predictive features, it is not useful in the context of online or interactive analytics. It is particularly useful in high dimensional unstructured data like language or images, where the challenge is not only to build models that fit well on seen data points, but also generalizes well to hitherto unseen data points.
              </p>
              <br>
              <h5>Datastores that make sense of data</h5>
              <p>
                  The next logical step would be datastores and systems that can make sense of data. Making sense of data would mean, instead of blindly pulling out data points such that variable X is x1 and Y to y2, it should also be able to interactively answer different class of queries like
                  <ul>
                      <li>
                          Give the best value for variable Y,  that maximizes the chance that X is x1.
                      </li>
                      <li>
                          Find all the variables or combination of variables, that influences X most when X is x1.
                      </li>
                  </ul>
              </p>
              <br>
              <p>
                  Such a system would continuously build a complete statistical or probabilistic model as and when data gets added or updated. Models would be descriptive and queryable. The time taken to infer or answer the different class of queries should also be tractable.  But just like there are a host of databases each tuned differently for 
                  <ul>
                    <li>Data Model</li>
                    <li>Scale</li>
                    <li>Read and Write Latencies</li>
                    <li>Transaction guarantees</li>
                    <li>Consistency, etc </li>
                  </ul>
              </p>
              <br>
              <p>
                We could possibly have different systems that makes sense of data based on the
                <ul>
                  <li>Assumptions on Data Model</li>
                  <li>Accuracy</li>
                  <li>Ability to Generalize</li>
                  <li>Scale of the data</li>
                  <li>Size of the models</li>
                  <li>Time taken to evaluate different types of queries.</li>
                </ul>
              </p>
              <br>
              <p>
                  Autometa, is one such, first of it’s kind system that we are building at factors.ai. It continuously makes sense of customer data to reduce the work involved in inferring from data. Give it a try! Or just drop in a mail to know more. 
              </p>
            </div>
          </div>
          <div class="blog-comment">
            <div id="disqus_thread"></div>
          </div>
        </div>
      </div>
  </div>
</div>

<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
var disqus_config = function () {
this.page.url = "www.factors.ai/#/blog/big-data-analytics-next-2";  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = "big-data-analytics-next-2"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://factorsai.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>